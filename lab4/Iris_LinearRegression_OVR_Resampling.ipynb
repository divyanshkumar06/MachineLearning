{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e297d8",
   "metadata": {},
   "source": [
    "\n",
    "# Iris Classification with Linear Regression + Resampling (Oversampling & SMOTE)\n",
    "\n",
    "**Goal:** Follow the assignment:\n",
    "1. Split the Iris dataset into training, validation, and test sets (80%/10%/10% and 70%/15%/15%).\n",
    "2. Create a **linear regression** model that predicts categories using a One-vs-Rest (OvR) approach.\n",
    "3. Make one class artificially minority by downsampling, then balance the training set with:\n",
    "   - **Random oversampling** (duplicate minority samples).\n",
    "   - **SMOTE** (interpolate synthetic samples) with two settings:\n",
    "     - `k_neighbors=1` (pair-based interpolation)\n",
    "     - `k_neighbors=5` (default-like nearest-neighbor interpolation)\n",
    "4. Train classifiers for each minority-choice and compare their performance.\n",
    "5. Evaluate on *validation* and *test* (kept untouched) and visualize confusions.\n",
    "\n",
    "> **Why linear regression for classification?**  \n",
    "> This is for the assignment. We build separate regressors for each class (target is 1 if sample belongs to class, else 0). At prediction, we pick the class whose regressor outputs the highest score.\n",
    "> This is called an **OvR Linear Regression** classifier. It's not the most robust classifier for multi-class problems (logistic regression or SVM are better), but it demonstrates the pipeline and resampling effects clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1eda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6872d",
   "metadata": {},
   "source": [
    "\n",
    "### Helper functions\n",
    "\n",
    "We encapsulate utilities to keep the notebook clean:\n",
    "- `split_train_val_test`: stratified splitting into train/val/test using two-stage split.\n",
    "- `OVRLinearRegression`: one-vs-rest linear regression wrapper.\n",
    "- `evaluate`: accuracy, macro-F1, and confusion matrix.\n",
    "- `class_counts` & `plot_confusion_matrix`: basic visualizations without seaborn.\n",
    "- `downsample_one_class`: create intentional class imbalance for experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa12655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_val_test(X, y, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, random_state=42):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=(1 - train_ratio), stratify=y, random_state=random_state\n",
    "    )\n",
    "    val_size = val_ratio / (val_ratio + test_ratio)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=(1 - val_size), stratify=y_temp, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "class OVRLinearRegression:\n",
    "    def __init__(self):\n",
    "        self.models_ = {}\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        for k in self.classes_:\n",
    "            y_binary = (y == k).astype(float)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y_binary)\n",
    "            self.models_[k] = model\n",
    "        return self\n",
    "\n",
    "    def predict_scores(self, X):\n",
    "        scores = []\n",
    "        for k in self.classes_:\n",
    "            s = self.models_[k].predict(X)\n",
    "            scores.append(s.reshape(-1, 1))\n",
    "        return np.hstack(scores)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.predict_scores(X)\n",
    "        idx = np.argmax(scores, axis=1)\n",
    "        return self.classes_[idx]\n",
    "\n",
    "def evaluate(y_true, y_pred, class_names):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    return acc, report, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def class_counts(y, class_names, title):\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    plt.bar(range(len(class_names)), counts.values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return counts\n",
    "\n",
    "def downsample_one_class(X, y, target_class, keep_n=10, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    mask_target = (y == target_class)\n",
    "    idx_all = np.arange(len(y))\n",
    "    idx_target = idx_all[mask_target]\n",
    "    idx_other = idx_all[~mask_target]\n",
    "    if keep_n >= len(idx_target):\n",
    "        chosen_target = idx_target\n",
    "    else:\n",
    "        chosen_target = rng.choice(idx_target, size=keep_n, replace=False)\n",
    "    new_idx = np.concatenate([chosen_target, idx_other])\n",
    "    rng.shuffle(new_idx)\n",
    "    return X[new_idx], y[new_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3b444",
   "metadata": {},
   "source": [
    "\n",
    "### Pipeline: load → split → scale → baseline → imbalance + resampling → compare\n",
    "\n",
    "We will run the full pipeline for both 80/10/10 and 70/15/15 splits.  \n",
    "For each split:\n",
    "1. Train a **baseline** OvR Linear Regression (no resampling).\n",
    "2. For each class (setosa, versicolor, virginica):\n",
    "   - Make it minority by downsampling to 10 samples in the training set.\n",
    "   - Balance using:\n",
    "     - **RandomOverSampler** (duplicates).\n",
    "     - **SMOTE** with `k_neighbors=1` (pair-based) and `k_neighbors=5` (nearest-neighbor).\n",
    "3. Evaluate on validation and test (unmodified) and record metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "def run_pipeline_for_split(train_ratio, val_ratio, test_ratio, random_state=42):\n",
    "    iris = load_iris(as_frame=True)\n",
    "    df = iris.frame.copy()\n",
    "    X = df[iris.feature_names].values\n",
    "    y = df[\"target\"].values\n",
    "    class_names = list(iris.target_names)\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(\n",
    "        X, y, train_ratio, val_ratio, test_ratio, random_state=random_state\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_val_s = scaler.transform(X_val)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Baseline\n",
    "    base = OVRLinearRegression().fit(X_train_s, y_train)\n",
    "    y_val_pred = base.predict(X_val_s)\n",
    "    y_test_pred = base.predict(X_test_s)\n",
    "    val_acc, val_report, val_cm = evaluate(y_val, y_val_pred, class_names)\n",
    "    test_acc, test_report, test_cm = evaluate(y_test, y_test_pred, class_names)\n",
    "\n",
    "    rows.append({\n",
    "        \"split\": f\"{int(train_ratio*100)}/{int(val_ratio*100)}/{int(test_ratio*100)}\",\n",
    "        \"scenario\": \"Baseline (no resampling)\",\n",
    "        \"minority_class\": \"N/A\",\n",
    "        \"val_accuracy\": val_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"val_macro_f1\": val_report[\"macro avg\"][\"f1-score\"],\n",
    "        \"test_macro_f1\": test_report[\"macro avg\"][\"f1-score\"]\n",
    "    })\n",
    "\n",
    "    plot_confusion_matrix(test_cm, class_names, f\"Confusion Matrix - Baseline (Test) [{int(train_ratio*100)}/{int(val_ratio*100)}/{int(test_ratio*100)}]\")\n",
    "\n",
    "    # RandomOverSampler experiments\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        X_train_imb, y_train_imb = downsample_one_class(X_train, y_train, target_class=cls_idx, keep_n=10, random_state=random_state)\n",
    "        X_train_imb_s = scaler.transform(X_train_imb)\n",
    "\n",
    "        ros = RandomOverSampler(random_state=random_state)\n",
    "        X_ros, y_ros = ros.fit_resample(X_train_imb_s, y_train_imb)\n",
    "\n",
    "        clf = OVRLinearRegression().fit(X_ros, y_ros)\n",
    "        y_val_pred = clf.predict(X_val_s)\n",
    "        y_test_pred = clf.predict(X_test_s)\n",
    "        val_acc, val_report, val_cm = evaluate(y_val, y_val_pred, class_names)\n",
    "        test_acc, test_report, test_cm = evaluate(y_test, y_test_pred, class_names)\n",
    "\n",
    "        rows.append({\n",
    "            \"split\": f\"{int(train_ratio*100)}/{int(val_ratio*100)}/{int(test_ratio*100)}\",\n",
    "            \"scenario\": \"RandomOverSampler\",\n",
    "            \"minority_class\": cls_name,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"val_macro_f1\": val_report[\"macro avg\"][\"f1-score\"],\n",
    "            \"test_macro_f1\": test_report[\"macro avg\"][\"f1-score\"]\n",
    "        })\n",
    "\n",
    "    # SMOTE experiments\n",
    "    for k_neighbors, label in [(1, \"SMOTE (k_neighbors=1)\"), (5, \"SMOTE (k_neighbors=5)\")]:\n",
    "        for cls_idx, cls_name in enumerate(class_names):\n",
    "            X_train_imb, y_train_imb = downsample_one_class(X_train, y_train, target_class=cls_idx, keep_n=10, random_state=random_state)\n",
    "            X_train_imb_s = scaler.transform(X_train_imb)\n",
    "\n",
    "            smote = SMOTE(random_state=random_state, k_neighbors=k_neighbors)\n",
    "            X_sm, y_sm = smote.fit_resample(X_train_imb_s, y_train_imb)\n",
    "\n",
    "            clf = OVRLinearRegression().fit(X_sm, y_sm)\n",
    "            y_val_pred = clf.predict(X_val_s)\n",
    "            y_test_pred = clf.predict(X_test_s)\n",
    "            val_acc, val_report, val_cm = evaluate(y_val, y_val_pred, class_names)\n",
    "            test_acc, test_report, test_cm = evaluate(y_test, y_test_pred, class_names)\n",
    "\n",
    "            rows.append({\n",
    "                \"split\": f\"{int(train_ratio*100)}/{int(val_ratio*100)}/{int(test_ratio*100)}\",\n",
    "                \"scenario\": label,\n",
    "                \"minority_class\": cls_name,\n",
    "                \"val_accuracy\": val_acc,\n",
    "                \"test_accuracy\": test_acc,\n",
    "                \"val_macro_f1\": val_report[\"macro avg\"][\"f1-score\"],\n",
    "                \"test_macro_f1\": test_report[\"macro avg\"][\"f1-score\"]\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "results_80 = run_pipeline_for_split(0.8, 0.1, 0.1, random_state=42)\n",
    "results_70 = run_pipeline_for_split(0.7, 0.15, 0.15, random_state=42)\n",
    "all_results = pd.concat([results_80, results_70], ignore_index=True).sort_values(by=[\"split\", \"scenario\", \"minority_class\"]).reset_index(drop=True)\n",
    "all_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade0ede",
   "metadata": {},
   "source": [
    "\n",
    "### How to read the results\n",
    "\n",
    "- **Baseline**: Model trained on the original (balanced) Iris training set.  \n",
    "- **RandomOverSampler**: We artificially made one class minority by downsampling it to 10 samples in the **training** set, then oversampled (duplicated) until classes balanced.  \n",
    "- **SMOTE**: Same imbalance as above, but we synthetically generate in-between samples instead of duplicates.\n",
    "  - `k_neighbors=1` approximates the instruction \"take any two samples and interpolate\".\n",
    "  - `k_neighbors=5` uses the nearest-neighbor set, generating more diverse synthetic samples.\n",
    "\n",
    "- The **validation/test** sets are **never resampled** and remain balanced; they reflect generalization.\n",
    "- Use **accuracy** and **macro-F1** to compare; macro-F1 weighs each class equally (good for class imbalance checks).\n",
    "- Check **confusion matrices** (plotted above) to see which classes are confused.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
